{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd190b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, EncoderDecoderModel, T5Config\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "import sentencepiece\n",
    "from rdkit import Chem\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3899bc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/sagawa/anaconda3/lib/python3.9/site-packages (from gdown) (3.6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/sagawa/anaconda3/lib/python3.9/site-packages (from gdown) (4.11.1)\n",
      "Requirement already satisfied: tqdm in /home/sagawa/anaconda3/lib/python3.9/site-packages (from gdown) (4.64.0)\n",
      "Requirement already satisfied: six in /home/sagawa/anaconda3/lib/python3.9/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in /home/sagawa/anaconda3/lib/python3.9/site-packages (from gdown) (2.27.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/sagawa/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/sagawa/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sagawa/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sagawa/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sagawa/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.9)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/sagawa/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14933 sha256=c991d8dce4891acd40973ec6ce4fcfce5a898cf1dfb8126218b3705186a086bd\n",
      "  Stored in directory: /home/sagawa/.cache/pip/wheels/b8/79/f0/b523d25d96b0bbb12bb024b97940d08c4fcd498a00070c8d82\n",
      "Successfully built gdown\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-4.5.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?export=download&id=1H4kOZpUayA-xSp0HNYRW4YXUcRhM9lhD\n",
      "To: /data1/sagawa_notebook/transformer-chemical-reaction-prediciton/16_p0.smi.gz\n",
      "100%|████████████████████████████████████████| 133M/133M [00:01<00:00, 68.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!gdown \"https://drive.google.com/uc?export=download&id=1H4kOZpUayA-xSp0HNYRW4YXUcRhM9lhD\"\n",
    "!gdown \"https://drive.google.com/uc?export=download&id=1DrDEjFNkU1YeubYw94k4K0Sek2xx1Sg1\"\n",
    "!gdown \"https://drive.google.com/uc?export=download&id=1NPpxUgPiCd_XPC37WvD6crZ5phRxB-ie\"\n",
    "!gdown \"https://drive.google.com/uc?export=download&id=1aJevXiOF8A6t9tMbfq2bmqoIsBfMyPAZ\"\n",
    "!gdown \"https://drive.google.com/uc?export=download&id=1ygYs8dy1-vxD1Vx6Ux7ftrXwZctFjpV3\"\n",
    "!gdown \"https://drive.google.com/uc?export=download&id=1BEk2GWhNU-Azj9hm77Z2wufsPN49wN0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dafd4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip -dr ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66446009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize(mol):\n",
    "    mol = Chem.MolToSmiles(Chem.MolFromSmiles(mol),True)\n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388afb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:44:28] Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "[10:44:28] ERROR: Could not sanitize molecule on line 49449\n",
      "[10:44:28] ERROR: Explicit valence for atom # 14 N, 4, is greater than permitted\n",
      "[10:44:43] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[10:44:43] ERROR: Could not sanitize molecule on line 116112\n",
      "[10:44:43] ERROR: Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[10:48:46] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[10:48:46] ERROR: Could not sanitize molecule on line 999334\n",
      "[10:48:46] ERROR: Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[10:48:46] Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[10:48:46] ERROR: Could not sanitize molecule on line 999335\n",
      "[10:48:46] ERROR: Explicit valence for atom # 13 N, 4, is greater than permitted\n",
      "[10:55:14] Explicit valence for atom # 4 N, 5, is greater than permitted\n",
      "[10:55:14] ERROR: Could not sanitize molecule on line 2347550\n",
      "[10:55:14] ERROR: Explicit valence for atom # 4 N, 5, is greater than permitted\n",
      "[10:55:14] Explicit valence for atom # 10 N, 5, is greater than permitted\n",
      "[10:55:14] ERROR: Could not sanitize molecule on line 2347629\n",
      "[10:55:14] ERROR: Explicit valence for atom # 10 N, 5, is greater than permitted\n",
      "[10:55:14] Explicit valence for atom # 10 N, 5, is greater than permitted\n",
      "[10:55:14] ERROR: Could not sanitize molecule on line 2348399\n",
      "[10:55:14] ERROR: Explicit valence for atom # 10 N, 5, is greater than permitted\n",
      "[10:58:07] Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[10:58:07] ERROR: Could not sanitize molecule on line 2924111\n",
      "[10:58:07] ERROR: Explicit valence for atom # 10 N, 4, is greater than permitted\n",
      "[11:00:01] Explicit valence for atom # 6 N, 5, is greater than permitted\n",
      "[11:00:01] ERROR: Could not sanitize molecule on line 3310125\n",
      "[11:00:01] ERROR: Explicit valence for atom # 6 N, 5, is greater than permitted\n",
      "[11:00:01] Explicit valence for atom # 9 N, 5, is greater than permitted\n",
      "[11:00:01] ERROR: Could not sanitize molecule on line 3310614\n",
      "[11:00:01] ERROR: Explicit valence for atom # 9 N, 5, is greater than permitted\n",
      "[11:00:01] Explicit valence for atom # 9 N, 5, is greater than permitted\n",
      "[11:00:01] ERROR: Could not sanitize molecule on line 3310734\n",
      "[11:00:01] ERROR: Explicit valence for atom # 9 N, 5, is greater than permitted\n",
      "[11:00:01] Explicit valence for atom # 7 N, 5, is greater than permitted\n",
      "[11:00:01] ERROR: Could not sanitize molecule on line 3311882\n",
      "[11:00:01] ERROR: Explicit valence for atom # 7 N, 5, is greater than permitted\n",
      "[11:00:26] Explicit valence for atom # 9 N, 5, is greater than permitted\n",
      "[11:00:26] ERROR: Could not sanitize molecule on line 3398197\n",
      "[11:00:26] ERROR: Explicit valence for atom # 9 N, 5, is greater than permitted\n",
      "[11:00:41] Explicit valence for atom # 6 N, 5, is greater than permitted\n",
      "[11:00:41] ERROR: Could not sanitize molecule on line 3449979\n",
      "[11:00:41] ERROR: Explicit valence for atom # 6 N, 5, is greater than permitted\n",
      "[11:00:47] Explicit valence for atom # 3 N, 5, is greater than permitted\n",
      "[11:00:47] ERROR: Could not sanitize molecule on line 3471482\n",
      "[11:00:47] ERROR: Explicit valence for atom # 3 N, 5, is greater than permitted\n",
      "[11:27:22] Explicit valence for atom # 10 N, 5, is greater than permitted\n",
      "[11:27:22] ERROR: Could not sanitize molecule on line 9442760\n",
      "[11:27:22] ERROR: Explicit valence for atom # 10 N, 5, is greater than permitted\n"
     ]
    }
   ],
   "source": [
    "lst = set([])\n",
    "for i in range(4):\n",
    "    suppl = Chem.SmilesMolSupplier('16_p0.smi')\n",
    "    for mol in suppl:\n",
    "        try:\n",
    "            sm = Chem.MolToSmiles(mol)\n",
    "            lst.add(sm)\n",
    "        except:\n",
    "            pass\n",
    "df = pd.DataFrame({'smiles': list(lst)})\n",
    "df.to_csv('./ZINC-canonicalized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e166994",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = pd.read_csv('./pubchem-10m.txt', names=['smiles'])\n",
    "lst = set([])\n",
    "for smiles in tx['smiles']:\n",
    "    try:\n",
    "        st.add(canonicalize(smiles))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69db542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412bbb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea7e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0e3d150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-773479c964d04b75\n",
      "Reusing dataset csv (/home/sagawa/.cache/huggingface/datasets/csv/default-773479c964d04b75/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360a71772a9a4a5f9ab9a98538d4276c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./pretraining/PubChem10m-deberta/PubChem10m-deberta-base/tokenizer_config.json',\n",
       " './pretraining/PubChem10m-deberta/PubChem10m-deberta-base/special_tokens_map.json',\n",
       " './pretraining/PubChem10m-deberta/PubChem10m-deberta-base/vocab.json',\n",
       " './pretraining/PubChem10m-deberta/PubChem10m-deberta-base/merges.txt',\n",
       " './pretraining/PubChem10m-deberta/PubChem10m-deberta-base/added_tokens.json',\n",
       " './pretraining/PubChem10m-deberta/PubChem10m-deberta-base/tokenizer.json')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PubChem10m data\n",
    "\n",
    "# Initialize a dataset\n",
    "dataset = load_dataset('csv',data_files='./data/pubchem-10m-canonicalized.csv')\n",
    "training_corpus = (\n",
    "    dataset['train'][i : i + 1000]['smiles']\n",
    "    for i in range(0, len(dataset['train']), 1000)\n",
    ")\n",
    "\n",
    "# Train tokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 1000)\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_pretrained('./pretraining/PubChem10m-deberta/PubChem10m-deberta-base')\n",
    "\n",
    "# Split into train and validation data\n",
    "all = pd.read_csv('./data/pubchem-10m-canonicalized.csv')\n",
    "train, valid = train_test_split(all, test_size=0.1)\n",
    "\n",
    "# Save train and validation data\n",
    "train.to_csv('./data/pubchem-10m-canonicalized-train.csv', index=False)\n",
    "valid.to_csv('./data/pubchem-10m-canonicalized-valid.csv', index=False)\n",
    "\n",
    "with open('./data/pubchem-10m-train.json', 'w') as fp:\n",
    "    for smiles in train.smiles:\n",
    "        fp.write(json.dumps({'text':smiles})+'\\n')\n",
    "\n",
    "with open('./data/pubchem-10m-valid.json', 'w') as fp:\n",
    "    for smiles in valid.smiles:\n",
    "        fp.write(json.dumps({'text':smiles})+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a60d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZINC data\n",
    "\n",
    "# Initialize a dataset\n",
    "dataset = load_dataset('csv',data_files='./data/ZINC-canonicalized.csv')\n",
    "training_corpus = (\n",
    "    dataset[\"train\"][i : i + 1000]['text']\n",
    "    for i in range(0, len(dataset[\"train\"]), 1000)\n",
    ")\n",
    "\n",
    "# Train tokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 1000)\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_pretrained('./pretraining/ZINC-deberta/ZINC-deberta-base')\n",
    "\n",
    "# Split into train and validation data\n",
    "all = pd.read_csv('./data/ZINC-canonicalized.csv')\n",
    "train, valid = train_test_split(all, test_size=0.1)\n",
    "\n",
    "# Save train and validation data\n",
    "train.to_csv('./data/ZINC-canonicalized-train.csv', index=False)\n",
    "valid.to_csv('./data/ZINC-canonicalized-valid.csv', index=False)\n",
    "\n",
    "with open(\"./data/ZINC-train.json\", \"w\") as fp:\n",
    "    for smiles in train.text:\n",
    "        fp.write(json.dumps({\"text\":smiles})+'\\n')\n",
    "\n",
    "with open(\"./data/ZINC-valid.json\", \"w\") as fp:\n",
    "    for smiles in valid.text:\n",
    "        fp.write(json.dumps({\"text\":smiles})+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4f7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/t5_tokenizer_model.py\n",
    "#!/usr/bin/env python3\n",
    "import json\n",
    "from typing import Iterator, List, Union\n",
    "\n",
    "from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers, trainers\n",
    "from tokenizers.implementations.base_tokenizer import BaseTokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "\n",
    "class SentencePieceUnigramTokenizer(BaseTokenizer):\n",
    "    \"\"\"\n",
    "    This class is a copy of `DeDLOC's tokenizer implementation <https://github.com/yandex-research/DeDLOC/blob/main/sahajbert/tokenizer/tokenizer_model.py>`__ .\n",
    "\n",
    "    Custom SentencePiece Unigram Tokenizer with NMT, NKFC, spaces and lower-casing characters normalization\n",
    "    Represents the Unigram algorithm, with the pretokenization used by SentencePiece\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        replacement: str = \"▁\",\n",
    "        add_prefix_space: bool = True,\n",
    "        unk_token: Union[str, AddedToken] = \"<unk>\",\n",
    "        eos_token: Union[str, AddedToken] = \"</s>\",\n",
    "        pad_token: Union[str, AddedToken] = \"<pad>\",\n",
    "    ):\n",
    "        self.special_tokens = {\n",
    "            \"pad\": {\"id\": 0, \"token\": pad_token},\n",
    "            \"eos\": {\"id\": 1, \"token\": eos_token},\n",
    "            \"unk\": {\"id\": 2, \"token\": unk_token},\n",
    "        }\n",
    "\n",
    "        self.special_tokens_list = [None] * len(self.special_tokens)\n",
    "        for token_dict in self.special_tokens.values():\n",
    "            self.special_tokens_list[token_dict[\"id\"]] = token_dict[\"token\"]\n",
    "\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "\n",
    "        tokenizer.normalizer = normalizers.Sequence(\n",
    "            [\n",
    "                normalizers.Nmt(),\n",
    "                normalizers.NFKC(),\n",
    "                normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "#                 normalizers.Lowercase(),\n",
    "            ]\n",
    "        )\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "            [\n",
    "                pre_tokenizers.Metaspace(replacement=replacement, add_prefix_space=add_prefix_space),\n",
    "                pre_tokenizers.Digits(individual_digits=True),\n",
    "                pre_tokenizers.Punctuation(),\n",
    "            ]\n",
    "        )\n",
    "        tokenizer.decoder = decoders.Metaspace(replacement=replacement, add_prefix_space=add_prefix_space)\n",
    "\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=f\"$A {self.special_tokens['eos']['token']}\",\n",
    "            special_tokens=[(self.special_tokens[\"eos\"][\"token\"], self.special_tokens[\"eos\"][\"id\"])],\n",
    "        )\n",
    "\n",
    "        parameters = {\n",
    "            \"model\": \"SentencePieceUnigram\",\n",
    "            \"replacement\": replacement,\n",
    "            \"add_prefix_space\": add_prefix_space,\n",
    "        }\n",
    "\n",
    "        super().__init__(tokenizer, parameters)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        files: Union[str, List[str]],\n",
    "        vocab_size: int = 8000,\n",
    "        show_progress: bool = True,\n",
    "    ):\n",
    "        \"\"\"Train the model using the given files\"\"\"\n",
    "\n",
    "        trainer = trainers.UnigramTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            special_tokens=self.special_tokens_list,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "        if isinstance(files, str):\n",
    "            files = [files]\n",
    "        self._tokenizer.train(files, trainer=trainer)\n",
    "\n",
    "        self.add_unk_id()\n",
    "\n",
    "    def train_from_iterator(\n",
    "        self,\n",
    "        iterator: Union[Iterator[str], Iterator[Iterator[str]]],\n",
    "        vocab_size: int = 8000,\n",
    "        show_progress: bool = True,\n",
    "    ):\n",
    "        \"\"\"Train the model using the given iterator\"\"\"\n",
    "\n",
    "        trainer = trainers.UnigramTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            special_tokens=self.special_tokens_list,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "        self._tokenizer.train_from_iterator(iterator, trainer=trainer)\n",
    "\n",
    "        self.add_unk_id()\n",
    "\n",
    "    def add_unk_id(self):\n",
    "        tokenizer_json = json.loads(self._tokenizer.to_str())\n",
    "\n",
    "        tokenizer_json[\"model\"][\"unk_id\"] = self.special_tokens[\"unk\"][\"id\"]\n",
    "\n",
    "        self._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93751d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e3ba6cdfe4878f3e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/sagawa/.cache/huggingface/datasets/csv/default-e3ba6cdfe4878f3e/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91da746981f43c7a32279405369bb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f9de7678c54991a8e32eb4f755e6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/sagawa/.cache/huggingface/datasets/csv/default-e3ba6cdfe4878f3e/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b02de6a852c40db904a11e3a138e9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PubChem10m data\n",
    "\n",
    "vocab_size = 32_000\n",
    "input_sentence_size = None\n",
    "\n",
    "# Initialize a dataset\n",
    "dataset = load_dataset('csv',data_files='./data/pubchem-10m-canonicalized.csv')\n",
    "\n",
    "tokenizer = SentencePieceUnigramTokenizer(unk_token=\"<unk>\", eos_token=\"</s>\", pad_token=\"<pad>\")\n",
    "\n",
    "# Build an iterator over this dataset\n",
    "def batch_iterator(input_sentence_size=None):\n",
    "    if input_sentence_size is None:\n",
    "        input_sentence_size = len(dataset)\n",
    "    batch_length = 1000\n",
    "    for i in range(0, input_sentence_size, batch_length):\n",
    "        yield dataset[\"train\"][i: i + batch_length]['smiles']\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(input_sentence_size=input_sentence_size),\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save('./pretraining/PubChem10m-t5/PubChem10m-t5-base/tokenizer.json')\n",
    "\n",
    "config = T5Config.from_pretrained('google/t5-v1_1-base', vocab_size=tokenizer.get_vocab_size())\n",
    "config.save_pretrained('./pretraining/PubChem10m-t5/PubChem10m-t5-base/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963651e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fedd0784d354e91c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/sagawa/.cache/huggingface/datasets/csv/default-fedd0784d354e91c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56759ba651aa452380d016ee6dd98f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2784d360f42f4db8b24312283ce6b686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/sagawa/.cache/huggingface/datasets/csv/default-fedd0784d354e91c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b7bf83b4f64734b1ceea39ed6be0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ZINC data\n",
    "\n",
    "vocab_size = 32_000\n",
    "input_sentence_size = None\n",
    "\n",
    "# Initialize a dataset\n",
    "dataset = load_dataset('csv',data_files='./data/ZINC-canonicalized.csv')\n",
    "\n",
    "tokenizer = SentencePieceUnigramTokenizer(unk_token=\"<unk>\", eos_token=\"</s>\", pad_token=\"<pad>\")\n",
    "\n",
    "# Build an iterator over this dataset\n",
    "def batch_iterator(input_sentence_size=None):\n",
    "    if input_sentence_size is None:\n",
    "        input_sentence_size = len(dataset)\n",
    "    batch_length = 1000\n",
    "    for i in range(0, input_sentence_size, batch_length):\n",
    "        yield dataset[\"train\"][i: i + batch_length]['text']\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(input_sentence_size=input_sentence_size),\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save('./pretraining/ZINC-t5/ZINC-t5-base/tokenizer.json')\n",
    "\n",
    "config = T5Config.from_pretrained('google/t5-v1_1-base', vocab_size=tokenizer.get_vocab_size())\n",
    "config.save_pretrained('./pretraining/ZINC-t5/ZINC-t5-base/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da5f2a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/sagawa_notebook/transformer-chemical-reaction-prediciton\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392fb10d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
