{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd190b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, T5Config\n",
    "from datasets import load_dataset\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "import sentencepiece\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare train, validation and test data\n",
    "all = pd.read_csv('./data/all_ord_reaction_uniq_canonicalized.csv')\n",
    "\n",
    "train, test = train_test_split(all, test_size=int(len(all)*0.1))\n",
    "train, val = train_test_split(train, test_size=int(len(all)*0.1))\n",
    "\n",
    "train.to_csv('./data/all_ord_reaction_uniq_canonicalized-train.csv', index=False)\n",
    "val.to_csv('./data/all_ord_reaction_uniq_canonicalized-valid.csv', index=False)\n",
    "test.to_csv('./data/all_ord_reaction_uniq_canonicalized-test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0e3d150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-773479c964d04b75\n",
      "Reusing dataset csv (/home/sagawa/.cache/huggingface/datasets/csv/default-773479c964d04b75/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360a71772a9a4a5f9ab9a98538d4276c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./pretraining/PubChem10m-deberta/PubChem10m-deberta-base/tokenizer_config.json',\n",
       " './pretraining/PubChem10m-deberta/PubChem10m-deberta-base/special_tokens_map.json',\n",
       " './pretraining/PubChem10m-deberta/PubChem10m-deberta-base/vocab.json',\n",
       " './pretraining/PubChem10m-deberta/PubChem10m-deberta-base/merges.txt',\n",
       " './pretraining/PubChem10m-deberta/PubChem10m-deberta-base/added_tokens.json',\n",
       " './pretraining/PubChem10m-deberta/PubChem10m-deberta-base/tokenizer.json')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PubChem10m data\n",
    "\n",
    "# Initialize a dataset\n",
    "dataset = load_dataset('csv',data_files='./data/pubchem-10m-canonicalized.csv')\n",
    "training_corpus = (\n",
    "    dataset['train'][i : i + 1000]['smiles']\n",
    "    for i in range(0, len(dataset['train']), 1000)\n",
    ")\n",
    "\n",
    "# Train tokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 1000)\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_pretrained('./pretraining/PubChem10m-deberta/PubChem10m-deberta-base')\n",
    "\n",
    "# Split into train and validation data\n",
    "all = pd.read_csv('./data/pubchem-10m-canonicalized.csv')\n",
    "train, valid = train_test_split(all, test_size=0.1)\n",
    "\n",
    "# Save train and validation data\n",
    "train.to_csv('./data/pubchem-10m-canonicalized-train.csv', index=False)\n",
    "valid.to_csv('./data/pubchem-10m-canonicalized-valid.csv', index=False)\n",
    "\n",
    "with open('./data/pubchem-10m-train.json', 'w') as fp:\n",
    "    for smiles in train.smiles:\n",
    "        fp.write(json.dumps({'text':smiles})+'\\n')\n",
    "\n",
    "with open('./data/pubchem-10m-valid.json', 'w') as fp:\n",
    "    for smiles in valid.smiles:\n",
    "        fp.write(json.dumps({'text':smiles})+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a60d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZINC data\n",
    "\n",
    "# Initialize a dataset\n",
    "dataset = load_dataset('csv',data_files='./data/ZINC-canonicalized.csv')\n",
    "training_corpus = (\n",
    "    dataset[\"train\"][i : i + 1000]['text']\n",
    "    for i in range(0, len(dataset[\"train\"]), 1000)\n",
    ")\n",
    "\n",
    "# Train tokenizer\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 1000)\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save_pretrained('./pretraining/ZINC-deberta/ZINC-deberta-base')\n",
    "\n",
    "# Split into train and validation data\n",
    "all = pd.read_csv('./data/ZINC-canonicalized.csv')\n",
    "train, valid = train_test_split(all, test_size=0.1)\n",
    "\n",
    "# Save train and validation data\n",
    "train.to_csv('./data/ZINC-canonicalized-train.csv', index=False)\n",
    "valid.to_csv('./data/ZINC-canonicalized-valid.csv', index=False)\n",
    "\n",
    "with open(\"./data/ZINC-train.json\", \"w\") as fp:\n",
    "    for smiles in train.text:\n",
    "        fp.write(json.dumps({\"text\":smiles})+'\\n')\n",
    "\n",
    "with open(\"./data/ZINC-valid.json\", \"w\") as fp:\n",
    "    for smiles in valid.text:\n",
    "        fp.write(json.dumps({\"text\":smiles})+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4f7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/t5_tokenizer_model.py\n",
    "#!/usr/bin/env python3\n",
    "import json\n",
    "from typing import Iterator, List, Union\n",
    "\n",
    "from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers, trainers\n",
    "from tokenizers.implementations.base_tokenizer import BaseTokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "\n",
    "class SentencePieceUnigramTokenizer(BaseTokenizer):\n",
    "    \"\"\"\n",
    "    This class is a copy of `DeDLOC's tokenizer implementation <https://github.com/yandex-research/DeDLOC/blob/main/sahajbert/tokenizer/tokenizer_model.py>`__ .\n",
    "\n",
    "    Custom SentencePiece Unigram Tokenizer with NMT, NKFC, spaces and lower-casing characters normalization\n",
    "    Represents the Unigram algorithm, with the pretokenization used by SentencePiece\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        replacement: str = \"‚ñÅ\",\n",
    "        add_prefix_space: bool = True,\n",
    "        unk_token: Union[str, AddedToken] = \"<unk>\",\n",
    "        eos_token: Union[str, AddedToken] = \"</s>\",\n",
    "        pad_token: Union[str, AddedToken] = \"<pad>\",\n",
    "    ):\n",
    "        self.special_tokens = {\n",
    "            \"pad\": {\"id\": 0, \"token\": pad_token},\n",
    "            \"eos\": {\"id\": 1, \"token\": eos_token},\n",
    "            \"unk\": {\"id\": 2, \"token\": unk_token},\n",
    "        }\n",
    "\n",
    "        self.special_tokens_list = [None] * len(self.special_tokens)\n",
    "        for token_dict in self.special_tokens.values():\n",
    "            self.special_tokens_list[token_dict[\"id\"]] = token_dict[\"token\"]\n",
    "\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "\n",
    "        tokenizer.normalizer = normalizers.Sequence(\n",
    "            [\n",
    "                normalizers.Nmt(),\n",
    "                normalizers.NFKC(),\n",
    "                normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "#                 normalizers.Lowercase(),\n",
    "            ]\n",
    "        )\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "            [\n",
    "                pre_tokenizers.Metaspace(replacement=replacement, add_prefix_space=add_prefix_space),\n",
    "                pre_tokenizers.Digits(individual_digits=True),\n",
    "                pre_tokenizers.Punctuation(),\n",
    "            ]\n",
    "        )\n",
    "        tokenizer.decoder = decoders.Metaspace(replacement=replacement, add_prefix_space=add_prefix_space)\n",
    "\n",
    "        tokenizer.post_processor = TemplateProcessing(\n",
    "            single=f\"$A {self.special_tokens['eos']['token']}\",\n",
    "            special_tokens=[(self.special_tokens[\"eos\"][\"token\"], self.special_tokens[\"eos\"][\"id\"])],\n",
    "        )\n",
    "\n",
    "        parameters = {\n",
    "            \"model\": \"SentencePieceUnigram\",\n",
    "            \"replacement\": replacement,\n",
    "            \"add_prefix_space\": add_prefix_space,\n",
    "        }\n",
    "\n",
    "        super().__init__(tokenizer, parameters)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        files: Union[str, List[str]],\n",
    "        vocab_size: int = 8000,\n",
    "        show_progress: bool = True,\n",
    "    ):\n",
    "        \"\"\"Train the model using the given files\"\"\"\n",
    "\n",
    "        trainer = trainers.UnigramTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            special_tokens=self.special_tokens_list,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "        if isinstance(files, str):\n",
    "            files = [files]\n",
    "        self._tokenizer.train(files, trainer=trainer)\n",
    "\n",
    "        self.add_unk_id()\n",
    "\n",
    "    def train_from_iterator(\n",
    "        self,\n",
    "        iterator: Union[Iterator[str], Iterator[Iterator[str]]],\n",
    "        vocab_size: int = 8000,\n",
    "        show_progress: bool = True,\n",
    "    ):\n",
    "        \"\"\"Train the model using the given iterator\"\"\"\n",
    "\n",
    "        trainer = trainers.UnigramTrainer(\n",
    "            vocab_size=vocab_size,\n",
    "            special_tokens=self.special_tokens_list,\n",
    "            show_progress=show_progress,\n",
    "        )\n",
    "\n",
    "        self._tokenizer.train_from_iterator(iterator, trainer=trainer)\n",
    "\n",
    "        self.add_unk_id()\n",
    "\n",
    "    def add_unk_id(self):\n",
    "        tokenizer_json = json.loads(self._tokenizer.to_str())\n",
    "\n",
    "        tokenizer_json[\"model\"][\"unk_id\"] = self.special_tokens[\"unk\"][\"id\"]\n",
    "\n",
    "        self._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93751d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e3ba6cdfe4878f3e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/sagawa/.cache/huggingface/datasets/csv/default-e3ba6cdfe4878f3e/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91da746981f43c7a32279405369bb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f9de7678c54991a8e32eb4f755e6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/sagawa/.cache/huggingface/datasets/csv/default-e3ba6cdfe4878f3e/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b02de6a852c40db904a11e3a138e9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PubChem10m data\n",
    "\n",
    "vocab_size = 32_000\n",
    "input_sentence_size = None\n",
    "\n",
    "# Initialize a dataset\n",
    "dataset = load_dataset('csv',data_files='./data/pubchem-10m-canonicalized.csv')\n",
    "\n",
    "tokenizer = SentencePieceUnigramTokenizer(unk_token=\"<unk>\", eos_token=\"</s>\", pad_token=\"<pad>\")\n",
    "\n",
    "# Build an iterator over this dataset\n",
    "def batch_iterator(input_sentence_size=None):\n",
    "    if input_sentence_size is None:\n",
    "        input_sentence_size = len(dataset)\n",
    "    batch_length = 1000\n",
    "    for i in range(0, input_sentence_size, batch_length):\n",
    "        yield dataset[\"train\"][i: i + batch_length]['smiles']\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(input_sentence_size=input_sentence_size),\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save('./pretraining/PubChem10m-t5/PubChem10m-t5-base/tokenizer.json')\n",
    "\n",
    "config = T5Config.from_pretrained('google/t5-v1_1-base', vocab_size=tokenizer.get_vocab_size())\n",
    "config.save_pretrained('./pretraining/PubChem10m-t5/PubChem10m-t5-base/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "963651e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fedd0784d354e91c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/sagawa/.cache/huggingface/datasets/csv/default-fedd0784d354e91c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56759ba651aa452380d016ee6dd98f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2784d360f42f4db8b24312283ce6b686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/sagawa/.cache/huggingface/datasets/csv/default-fedd0784d354e91c/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b7bf83b4f64734b1ceea39ed6be0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ZINC data\n",
    "\n",
    "vocab_size = 32_000\n",
    "input_sentence_size = None\n",
    "\n",
    "# Initialize a dataset\n",
    "dataset = load_dataset('csv',data_files='./data/ZINC-canonicalized.csv')\n",
    "\n",
    "tokenizer = SentencePieceUnigramTokenizer(unk_token=\"<unk>\", eos_token=\"</s>\", pad_token=\"<pad>\")\n",
    "\n",
    "# Build an iterator over this dataset\n",
    "def batch_iterator(input_sentence_size=None):\n",
    "    if input_sentence_size is None:\n",
    "        input_sentence_size = len(dataset)\n",
    "    batch_length = 1000\n",
    "    for i in range(0, input_sentence_size, batch_length):\n",
    "        yield dataset[\"train\"][i: i + batch_length]['text']\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer.train_from_iterator(\n",
    "    iterator=batch_iterator(input_sentence_size=input_sentence_size),\n",
    "    vocab_size=vocab_size,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "# Save files to disk\n",
    "tokenizer.save('./pretraining/ZINC-t5/ZINC-t5-base/tokenizer.json')\n",
    "\n",
    "config = T5Config.from_pretrained('google/t5-v1_1-base', vocab_size=tokenizer.get_vocab_size())\n",
    "config.save_pretrained('./pretraining/ZINC-t5/ZINC-t5-base/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
