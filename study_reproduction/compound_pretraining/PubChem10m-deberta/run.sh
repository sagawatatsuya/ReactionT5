python ./run_mlm.py --model_name_or_path "microsoft/deberta-base" --tokenizer_name "./PubChem10m-deberta-base" --num_train_epochs 10 --dataset_name "sagawa/pubchem-10m-canonicalized" --per_device_train_batch_size 5 --max_seq_length 512 --do_train --do_eval --output_dir "./PubChem10m-deberta-base-output" --overwrite_output_dir --evaluation_strategy "steps" --eval_steps 100000 --save_strategy "steps" --save_steps="100000" --save_total_limit=2 --logging_steps 500 --learning_rate 0.00005 --report_to "none" --use_new_implementation --load_best_model_at_end True --disable_tqdm True