python ./new_run_t5_mlm_flax.py --output_dir="./PubChem10m-t5-base-output" --model_type="t5" --config_name="./PubChem10m-t5-base" --tokenizer_name="./PubChem10m-t5-base" --dataset_name "sagawa/pubchem-10m-canonicalized" --max_seq_length="512" --per_device_train_batch_size="5" --per_device_eval_batch_size="5" --adafactor --learning_rate="0.005" --weight_decay="0.001" --warmup_steps="2000" --overwrite_output_dir --logging_steps="500" --save_steps="100000" --num_train_epochs="30" --do_train --do_eval --eval_steps="100000"